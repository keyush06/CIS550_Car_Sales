# -*- coding: utf-8 -*-
"""CIS 550 MileStone3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xorvpZM9tXNf0GLdD_A_T2JaVq6kMRrD
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

used_cars = pd.read_csv('/content/drive/My Drive/550 project deliverables/vehicles.csv')
# car_sales = pd.read_csv('D:/UPenn/Spring\'24/CIS 5500 (DBMS)/Final Project/Datasets/Final_Datasets/car_prices.csv')
used_cars.head()
used_cars.shape

used_nan = used_cars.isna().sum()
# car_sales_nan = car_sales.isna().sum()
used_nan

used_cars.rename(columns={'VIN': 'vin', 'paint_color': 'color', 'image_url': 'image'}, inplace=True)

def check_nulls(X_data):
    # Define the unusual null values
    unusual_null_values = ['{}', '[]', '?', '.', '-', '_', '', ' ', '  ']

    # Initialize a DataFrame to store counts of unusual nulls
    unusual_nulls = pd.DataFrame(index=X_data.columns, columns=['strange_nulls'])
    unusual_nulls['strange_nulls'] = 0

    # Count the occurrences of each unusual null value in each column
    for value in unusual_null_values:
        unusual_nulls['strange_nulls'] += X_data.apply(lambda x: (x == value).sum())

    # Count the usual null values
    usual_nulls = X_data.isna().sum()

    # Combine the counts into a single DataFrame
    nulls_df = pd.concat([usual_nulls, unusual_nulls], axis=1)
    nulls_df.columns = ["usual_nulls", "strange_nulls"]
    nulls_df = nulls_df.sort_values('usual_nulls', ascending=False)

    return nulls_df

check_nulls(used_cars)

used_cars['type'].value_counts()

used_cars[['id','vin']].tail(10)

"""Replacing null values"""

used_cars.drop(['county','size'],axis = 1, inplace=True)

used_cars.columns

used_cars.posting_date.tail(10)

used_cars.condition.value_counts()

nan_count = used_cars['condition'].isna().sum()

# Calculate how many values to replace with 'good' and 'fair'
half_nan_count = nan_count // 2
nan_indices = used_cars[used_cars['condition'].isna()].index
good_indices = np.random.choice(nan_indices, size=half_nan_count, replace=False)

## Putting half as fair and half as good
used_cars.loc[good_indices, 'condition'] = 'good'

used_cars.loc[used_cars['condition'].isna(), 'condition'] = 'fair'

used_cars.condition.value_counts()

"""Since cylinder and color have "ohter", we can tag the null values as that"""

used_cars['cylinders'].fillna('other', inplace=True)
used_cars['color'].fillna('other', inplace=True)

used_cars['type'].fillna('other', inplace=True)

check_nulls(used_cars)

used_cars.drive.value_counts()

## Drive col in ratio 2:2:1

total_nans = used_cars['drive'].isna().sum()

# Dividng each one of them in ratios
ratios = {'4wd': 2, 'fwd': 2, 'rwd': 1}
total_ratio = sum(ratios.values())

replacements = {k: int((v / total_ratio) * total_nans) for k, v in ratios.items()}
replacements['rwd'] += total_nans - sum(replacements.values())
nan_indices = used_cars[used_cars['drive'].isna()].index

# Shuffle the NaN indices to randomize where replacements occur
nan_indices = used_cars[used_cars['drive'].isna()].index.to_numpy()
np.random.shuffle(nan_indices)

start_idx = 0
for drive_type, count in replacements.items():
    end_idx = start_idx + count
    used_cars.loc[nan_indices[start_idx:end_idx], 'drive'] = drive_type
    start_idx = end_idx

used_cars.drive.value_counts()

used_cars.shape

check_nulls(used_cars)

listing_columns = [
    'id', 'region', 'region_url', 'lat', 'long', 'state',
    'price', 'url', 'year', 'image', 'description', 'condition', 'color', 'odometer'
]

# Define the columns for the Cars DataFrame
cars_columns = [
    'vin', 'manufacturer', 'model', 'cylinders', 'transmission', 'fuel', 'type', 'drive'
]

# Create the Listing DataFrame
listing_df = used_cars[listing_columns]

# Create the Cars DataFrame
cars_df = used_cars[cars_columns]

"""**Directly removing the rows for other columns as they are significant given the szie of the dataset**"""

used_cars = used_cars.dropna(subset=['manufacturer', 'title_status',
                                     'long', 'lat', 'model', 'odometer', 'fuel',
                                     'transmission', 'year', 'description', 'image',
                                     'posting_date'])

used_cars.vin.value_counts()

vin_location_counts = used_cars.groupby(['vin', 'region']).size().reset_index(name='counts')

vin_location_counts_sorted = vin_location_counts.sort_values(by=['vin', 'counts'], ascending=[True, False])
final_df = vin_location_counts_sorted.drop_duplicates(subset='vin', keep='first')

final_df['vin'].value_counts()

xx = final_df.merge(used_cars, on=['vin', 'region'], how='left')

xx.vin.value_counts()

"""**We only keep one occurence of each of the VIN such that the region has the highest number of rows in the dataset. We do not want to lose a lot of columns.**"""

used_cars = pd.merge(final_df[['vin']], used_cars, on='vin', how='left')

# used_cars[used_cars['vin'] == '1FMJU1JT1HEA52352']
used_cars.shape

check_nulls(used_cars)

used_cars.posting_date.tail(20)

used_cars['posting_date'] = pd.to_datetime(used_cars['posting_date'], utc=True, errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')


print(used_cars['posting_date'].tail())



listing_df.to_csv('listing.csv', index=False)

# Save the Cars DataFrame to a CSV file
cars_df.to_csv('cars.csv', index=False)

listing_df.dtypes

cars_df.dtypes

listing_df.to_csv('listing_final.csv', index=False)

file_path = '/content/drive/My Drive/550 project deliverables/listing_final.csv'
used_cars.to_csv(file_path, index=False)

